{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUBES ML\n",
    "13515055 | Rizky Faramita  \n",
    "13515064 | Tasya  \n",
    "13515140 | Francisco Kenandi Cahyono  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris().data\n",
    "target = datasets.load_iris().target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Library & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances,manhattan_distances\n",
    "import numpy as np\n",
    "\n",
    "def calculateEucledianDistance(data):\n",
    "    return euclidean_distances(data)\n",
    "\n",
    "def calculateManhattanDistance(data):\n",
    "    return manhattan_distances(data)\n",
    "\n",
    "def eucledianDistance(a,b):\n",
    "    dist = 0\n",
    "    for i in range(len(a)):\n",
    "        dist += np.power((a[i]-b[i]),2)\n",
    "    return np.sqrt(dist)\n",
    "\n",
    "def manhattanDistance(a,b):\n",
    "    dist = 0\n",
    "    for i in range(len(a)):\n",
    "        dist += abs(a[i]-b[i])\n",
    "    return dist\n",
    "\n",
    "def checkWithScikit(customLabel,scikitLabel):\n",
    "    if len(customLabel) == len(scikitLabel):\n",
    "        same = True\n",
    "        for i in range(len(customLabel)):\n",
    "            if (customLabel[i] != scikitLabel[i]):\n",
    "                print(\"Oops, it's different.\")\n",
    "                same = False\n",
    "                break\n",
    "        if same: \n",
    "            print(\"Yes, it's the same!!\")\n",
    "    else:\n",
    "        print(\"Oops, it's different.\")\n",
    "\n",
    "def calculatePurity(true_label,predicted_label,ncluster):\n",
    "    n = len(true_label)\n",
    "    temp_sum = 0\n",
    "    n_label = []\n",
    "    for i in range (ncluster):\n",
    "        n_label.append(0)\n",
    "        \n",
    "    for i in range(ncluster):\n",
    "        n_label = [0 for k in range (len(n_label))]\n",
    "        for j in range(len(predicted_label)):\n",
    "            if predicted_label[j] == i:\n",
    "                n_label[i] += 1\n",
    "        temp_sum += max(n_label)\n",
    "            \n",
    "    return (temp_sum/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means merupakan salah satu metode *clustering* yang sensitif terhadap nilai inisialisasi. Pada metode ini, banyaknya kluster yang ingin dibuat sudah diketahui sejak awal.\n",
    "\n",
    "Pada dasarnya, akan diinisialisasi sentroid awal untuk masing-masing kluster. Kemudian, dihitung jarak dari setiap data ke setiap sentroid yang ada. Data tersebut akan dimasukkan ke dalam suatu klulster yang sentroidnya paling dekat dengan data.\n",
    "Kemudian, akan dihitung kembali sentroid setiap kluster dengan cara menghitung *mean*-nya.\n",
    "\n",
    "Langkah di atas akan diulang secara terus-menerus sehingga tidak terjadi perubahan sentroid ketika perhitungan *mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "1. Masukkan banyak kluster yang ingin dibuat dan pilih sentroid awal untuk masing-masing kluster.\n",
    "2. Ulangi: Masukkan (ulang) setiap data ke dalam kluster terdekat, kemudian *update* sentroid setiap kluster dengan cara menghitung *means*-nya.\n",
    "3. Berhenti ketika sentroid dari setiap kluster tidak ada yang berubah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kompleksitas dari algoritma K-Means adalah O(nkt) dengan t adalah jumlah iterasi, k adalah banyaknya kluster, dan n adalah banyaknya data yang ingin dikluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with custom KMeans\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1.]\n",
      "\n",
      "Clustering with sklearn's KMeans:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n",
      " 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n",
      " 0 2]\n",
      "\n",
      "Purity custom KMeans:  1.0\n",
      "Purity scikit's KMeans:  1.0\n"
     ]
    }
   ],
   "source": [
    "# KMeans Function Implementation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# calculate Euclidean distance  \n",
    "def euclDistance(vector1, vector2):  \n",
    "    return sqrt(sum(power(vector2 - vector1, 2)))\n",
    "  \n",
    "# init centroids with random samples  \n",
    "def initCentroids(dataSet, k):  \n",
    "    numSamples, dim = dataSet.shape\n",
    "    centroids = zeros((k, dim))\n",
    "    for i in range(k):  \n",
    "        index = int(random.uniform(0, numSamples))\n",
    "        centroids[i, :] = dataSet[index, :]  \n",
    "    return centroids  \n",
    "  \n",
    "# k-means cluster\n",
    "def kmeans(dataSet, k):  \n",
    "    numSamples = dataSet.shape[0]\n",
    "    clusterAssment = mat(zeros((numSamples, 2)))\n",
    "    clusterChanged = True  \n",
    "  \n",
    "    ## step 1: init centroids  \n",
    "    centroids = initCentroids(dataSet, k)\n",
    "  \n",
    "    while clusterChanged:  \n",
    "        clusterChanged = False  \n",
    "        ## for each sample  \n",
    "        for i in range(numSamples):\n",
    "            minDist  = 100000.0  \n",
    "            minIndex = 0  \n",
    "            ## for each centroid  \n",
    "            ## step 2: find the centroid who is closest\n",
    "            for j in range(k):  \n",
    "                distance = euclDistance(centroids[j, :], dataSet[i, :])  \n",
    "                if distance < minDist:  \n",
    "                    minDist  = distance  \n",
    "                    minIndex = j  \n",
    "              \n",
    "            ## step 3: update its cluster\n",
    "            if clusterAssment[i, 0] != minIndex:  \n",
    "                clusterChanged = True  \n",
    "                clusterAssment[i, :] = minIndex, minDist**2\n",
    "  \n",
    "        ## step 4: update centroids  \n",
    "        for j in range(k):\n",
    "            pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]\n",
    "            centroids[j, :] = mean(pointsInCluster, axis = 0)\n",
    "        \n",
    "        model = []\n",
    "        for c in clusterAssment[:,0]:\n",
    "            model = append(model, c)\n",
    "            \n",
    "    return model\n",
    "\n",
    "# OUR KMEANS\n",
    "kmeans_custom = kmeans(iris, 3)\n",
    "print(\"Clustering with custom KMeans\")\n",
    "print(kmeans_custom)\n",
    "print()\n",
    "\n",
    "# SCIKIT'S KMEANS\n",
    "kmeans_sklearn = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', \n",
    "                verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto').fit_predict(iris)\n",
    "print(\"Clustering with sklearn's KMeans:\")\n",
    "print(kmeans_sklearn)\n",
    "print()\n",
    "\n",
    "# Calculate Purity\n",
    "print(\"Purity custom KMeans: \", calculatePurity(target, kmeans_custom, 3))\n",
    "print(\"Purity scikit's KMeans: \", calculatePurity(target, kmeans_sklearn, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEDOIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Medoids pada dasarnya mirip dengan K-Means. Letak perbedaan K-Medoids dengan K-Means terletak pada *update* sentroid. Jika pada K-Means cara meng-*update* sentroid tiap kluster adalah dengan menghitung *means* dari kluster tersebut, namun ada K-Medoids akan diambil satu data untuk menggantikan salah satu sentroid yang ada. Dengan cara ini, K-Medoids lebih rigid terhadap pencilan (*outliers*) dibandingkan K-Means.\n",
    "\n",
    "Karena pemilihan data *random* memiliki banyak kemungkinan, biasanya K-Medoids akan berhenti setelah beberapa data yang dipilih menghasilkan kluster dengan *error* yang lebih besar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "1. Masukkan banyak kluster yang ingin dibuat dan pilih sentroid awal untuk masing-masing kluster.\n",
    "2. Ulangi: Masukkan (ulang) setiap data ke dalam kluster terdekat, kemudian *update* sentroid setiap kluster dengan cara memilih salah satu data *random* untuk menggantikan sentroid yang sudah ada.\n",
    "3. Berhenti ketika *error* yang dihasilkan minimal atau banyaknya data *random* yang ditemukan untuk menggantikan sentroid lainnya tidak menghasilkan *error* yang lebih kecil melebihi *threshold*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with custom KMedoids:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "\n",
      "Clustering with scikit's KMedoids:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n",
      "\n",
      "Purity custom KMedoids:  1.0\n",
      "Purity scikit's KMedoids:  1.0\n"
     ]
    }
   ],
   "source": [
    "# KMedoids Function Implementation\n",
    "from numpy import *  \n",
    "\n",
    "# calculate Manhattan distance  \n",
    "def manhatDistance(vector1, vector2):  \n",
    "    return sum(fabs(vector2 - vector1))\n",
    "  \n",
    "# init centroids with random samples  \n",
    "def initCentroids(dataSet, k):  \n",
    "    numSamples, dim = dataSet.shape\n",
    "    centroids = zeros((k, dim))\n",
    "    for i in range(k):  \n",
    "        index = int(random.uniform(0, numSamples))\n",
    "        centroids[i, :] = dataSet[index, :]  \n",
    "    return centroids\n",
    "\n",
    "def updateCentroids(dataSet, k, centroids, clusterAssment):\n",
    "    clust = int(random.uniform(0,k))\n",
    "    numSamples = dataSet.shape[0]\n",
    "    idx = int(random.uniform(0, numSamples))\n",
    "    while clusterAssment[idx, 0] != clust:\n",
    "        idx = int(random.uniform(0, numSamples))\n",
    "    centroids[clust,:] = dataSet[idx, :]\n",
    "    return centroids\n",
    "\n",
    "# k-medoids cluster\n",
    "def kmedoids(dataSet, k):  \n",
    "    numSamples = dataSet.shape[0]\n",
    "    clusterAssment = mat(zeros((numSamples, 2)))\n",
    "    clusterChanged = True  \n",
    "  \n",
    "    ## step 1: init centroids  \n",
    "    centroids = initCentroids(dataSet, k)\n",
    "    error = 0\n",
    "  \n",
    "    while clusterChanged:  \n",
    "        clusterChanged = False  \n",
    "        ## for each sample  \n",
    "        for i in range(numSamples):\n",
    "            minDist  = 100000.0  \n",
    "            minIndex = 0  \n",
    "            ## for each centroid  \n",
    "            ## step 2: find the centroid who is closest\n",
    "            for j in range(k):  \n",
    "                distance = manhatDistance(centroids[j, :], dataSet[i, :])\n",
    "                if distance < minDist:  \n",
    "                    minDist  = distance  \n",
    "                    minIndex = j  \n",
    "              \n",
    "            ## step 3: update its cluster\n",
    "            if clusterAssment[i, 0] != minIndex:  \n",
    "                clusterChanged = True  \n",
    "                clusterAssment[i, :] = minIndex, minDist\n",
    "  \n",
    "        ## step 4: update centroids\n",
    "        new_error = sum(clusterAssment[:, 1])\n",
    "        if new_error < error:\n",
    "            clusterChanged = False\n",
    "            error = new_error\n",
    "        else:\n",
    "            centroids = updateCentroids(dataSet,k,centroids,clusterAssment)\n",
    "        \n",
    "        model = []\n",
    "        for c in clusterAssment[:,0]:\n",
    "            model = append(model, c)\n",
    "    return model\n",
    "\n",
    "# OUR KMEDOIDS\n",
    "kmedoids_custom = kmedoids(iris, 3)\n",
    "print(\"Clustering with custom KMedoids:\")\n",
    "print(kmedoids_custom)\n",
    "print()\n",
    "\n",
    "# SCIKIT'S KMEDOIDS\n",
    "from pyclustering.cluster import kmedoids\n",
    "kmedoids_sklearn_init = kmedoids.kmedoids(iris, [0,50,100],ccore='True')\n",
    "kmedoids_sklearn_init.process()\n",
    "kmedoids_sklearn_model = kmedoids_sklearn_init.get_clusters()\n",
    "i = 0\n",
    "kmedoids_sklearn = np.arange(150)\n",
    "for c in kmedoids_sklearn_model:\n",
    "    for cj in c:\n",
    "        kmedoids_sklearn[cj] = i\n",
    "    i += 1\n",
    "print(\"Clustering with scikit's KMedoids:\")\n",
    "print(kmedoids_sklearn)\n",
    "print()\n",
    "\n",
    "# Calculate Purity\n",
    "print(\"Purity custom KMedoids: \", calculatePurity(target, kmedoids_custom, 3))\n",
    "print(\"Purity scikit's KMedoids: \", calculatePurity(target, kmedoids_sklearn, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN adalah algoritma yang termasuk ke dalam Density-based Clustering. Oleh karena itu, definisi kluster adalah daerah padat dalam ruang data yang dipisahkan oleh daerah dengan kepadatan objek yang lebih rendah. Sebuah area dapat disebut sebagai sebuah kluster apabila pada daerah tersebut terdapat data paling sedikit MinPts (didefinisikan pada awal klustering). *High density* adalah apabila di area dengan jari-jari epsilon, terdapat paling sedikit data sebanyak MinPts (epsilon didefinisikan di awal juga).\n",
    "\n",
    "Sebuah data disebut *core* data apabila di sekeliling data tersebut (dengan jari-jari epsilon) terdapat minimal MinPts banyak data. *Border* data adalah data yang bukan *core* namun data tersebut masih terletak di dekat *core* data. *Outlier* adalah data yang terpisah dari data-data lain atau yang sering disebut juga sebagai pencilan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "\n",
    "Algoritma dimulai dengan sembarang data D, kemudian mencari tetangga-tetangga di sekitar D yang dapat dicapai (yaitu masih dicakup pada jari=jari sebesar epsilon).\n",
    "a. Jika D adalah sebuah *core*, iterasi ini akan menghasilkan sebuah kluster.\n",
    "b. Jika D adalah sebuah *border*, tidak akan terbentuk kluster baru dan algoritma ini akan menghitung data lain dari kumpulan data yang ingin dilakukan klustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Function Implementation\n",
    "class customDBSCAN:\n",
    "    \n",
    "    def __init__(self,data,e,minPts,distanceFunction):\n",
    "        self.data = data\n",
    "        self.e = e\n",
    "        self.num_data = len(self.data)\n",
    "        self.minPts = minPts\n",
    "        self.labels = []\n",
    "        self.cores = {}\n",
    "        for x in range(len(self.data)):\n",
    "            self.labels.append(-1)\n",
    "        if distanceFunction == 'eucledian':\n",
    "            self.distanceMatrix = calculateEucledianDistance(self.data)\n",
    "        elif distanceFunction == 'manhattan':\n",
    "            self.distanceMatrix = calculateManhattanDistance(self.data)\n",
    "    def findCores(self):\n",
    "        for i in range(self.num_data):\n",
    "            temp_cluster = []\n",
    "            for j in range(self.num_data):\n",
    "                if (self.distanceMatrix[i][j] <= self.e and i != j):\n",
    "                    temp_cluster.append(j)\n",
    "            if(len(temp_cluster) >= self.minPts):\n",
    "                self.cores[i] = temp_cluster\n",
    "    \n",
    "    def dfs_dbscan(self,dict_type_data,current_label):\n",
    "        if(self.labels[dict_type_data] == -1):\n",
    "            self.labels[dict_type_data] = current_label\n",
    "            if dict_type_data in self.cores:\n",
    "                for i in self.cores[dict_type_data]:\n",
    "                    self.dfs_dbscan(i,current_label)\n",
    "        \n",
    "    def fit(self):\n",
    "        label = -1\n",
    "        self.findCores()\n",
    "        for k,v in self.cores.items():\n",
    "            if (self.labels[k] == -1):\n",
    "                label+=1\n",
    "            self.dfs_dbscan(k,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with custom DBSCAN: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Clustering with scikit's DBSCAN: \n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n",
      "\n",
      "Compare with scikit:\n",
      "Yes, it's the same!!\n",
      "\n",
      "Purity custom DBSCAN:  0.98\n",
      "Purity scikit's DBSCAN:  0.98\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "distanceFunction = 'eucledian'\n",
    "epsilon = 1\n",
    "minPts = 17\n",
    "\n",
    "# OUR DBSCAN\n",
    "db = customDBSCAN(iris,epsilon,minPts,distanceFunction)\n",
    "db.fit()\n",
    "print(\"Clustering with custom DBSCAN: \")\n",
    "print(db.labels)\n",
    "print()\n",
    "\n",
    "# SCIKIT'S DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=epsilon, min_samples=minPts).fit(iris)\n",
    "print(\"Clustering with scikit's DBSCAN: \")\n",
    "print(clustering.labels_)\n",
    "print()\n",
    "\n",
    "# Compare custom DBSCAN with SCIKIT's\n",
    "print(\"Compare with scikit:\")\n",
    "checkWithScikit(db.labels,clustering.labels_)\n",
    "print()\n",
    "\n",
    "# Calculate Purity\n",
    "print(\"Purity custom DBSCAN: \", calculatePurity(target,db.labels,len(set(db.labels))))\n",
    "print(\"Purity scikit's DBSCAN: \", calculatePurity(target,clustering.labels_,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGLOMERATIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative merupakan salah satu metode *hierarchical clustering*. Pada dasarnya, *hierarchical clustering* ini ada dua macam, yaitu *top-down* (dari satu kluster besar dipecah menjadi kluster-kluster kecil, disebut juga Divisive Clustering) dan *bottom-up* (dari banyak kluster kecil dijadikan satu kluster besar yang anggotanya adalah semua data yang ada, disebut Agglomerative Clustering). Sama seperti K-Means dan DBSCAN, Agglomerative juga butuh banyaknya kluster secara eksplisit dinyatakan ketika ingin melakukan klustering. Hal ini disebabkan oleh metode Agglomerative akan melakukan iterasi secara terus-menerus hingga membentuk satu kluster besar apabila tidak ada *threshold* banyaknya kluster yang harus dibuat. Dengan adanya batasan banyaknya kluster yang harus dibuat, maka algoritma ini akan berhenti hingga banyak kluster yang dibuat sudah sesuai dengan keinginan pengguna.\n",
    "\n",
    "Dalam satu iterasi, metode Agglomerative hanya akan menggabungkan satu data ke dalam kluster terdekat dengan dirinya. Cara perhitungan jarak pada algoritma ini ada 4 macam:\n",
    "1. Single: jarak antarkluster adalah jarak terdekat antara salah satu objek dari kedua kluster tersebut.\n",
    "2. Complete: jarak antarkluster adalah jarak terjauh antara salah satu objek dari kedua kluster tersebut.\n",
    "3. Average: jarak antarkluster adalah rata-rata jarak terdekat antara setiap data dari kedua kluster.\n",
    "4. Average group: jarak antara kedua sentroid (*means*) dari kedua kluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "\n",
    "1. Algoritma dimulai dengan adanya kluster kecil yang banyaknya sesuai dengan banyaknya data (satu kluster hanya berisi satu data).\n",
    "2. Cari jarak minimal berdasarkan salah satu algoritma jarak yang sudah dijelaskan di atas, gabungkan kluster dengan jarak paling dekat.\n",
    "3. *Update* matriks jarak dengan menghitung jarak antara kluster baru ke kluster-kluster lainnya.\n",
    "4. Ulang step 2 dan 3 hingga kluster yang ada sama dengan kluster yang diinginkan atau sampai terbentuk satu kluster besar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM AGGLOMERATIVE\n",
    "class customAgglomerative:\n",
    "    def __init__(self,data,n_cluster,linkage='single',distanceFunction='eucledian'):\n",
    "        self.data = data\n",
    "        self.n_cluster = n_cluster\n",
    "        \n",
    "        if linkage == 'single':\n",
    "            self.linkage = self.singleLinkage\n",
    "        elif linkage == 'complete':\n",
    "            self.linkage = self.completeLinkage\n",
    "        elif linkage == 'average':\n",
    "            self.linkage = self.averageLinkage\n",
    "        elif linkage == 'average_group':\n",
    "            self.linkage = self.averageGroupLinkage\n",
    "        \n",
    "        if distanceFunction == 'eucledian':\n",
    "            self.distanceFunction = eucledianDistance\n",
    "        elif distanceFunction == 'manhattan':\n",
    "            self.distanceFunction = manhattanDistance\n",
    "        \n",
    "        self.distances = {}\n",
    "        #clusters = list of tuples\n",
    "        self.clusters = [(idx,)for idx in range(len(self.data))]\n",
    "        #label of each data's cluster\n",
    "        self.labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            self.labels.append(i)\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        while len(self.clusters)>self.n_cluster:\n",
    "            a = 0\n",
    "            b = 0\n",
    "            clusterDistance = np.inf\n",
    "            \n",
    "            for i in range(len(self.clusters)):\n",
    "                for j in range(i+1, len(self.clusters)):\n",
    "                    temp_clusterDistance = self.getLinkageDistance(self.clusters[i],self.clusters[j])\n",
    "                    if temp_clusterDistance < clusterDistance:\n",
    "                        a,b,clusterDistance = i,j,temp_clusterDistance\n",
    "                        \n",
    "            self.clusters[a] = self.clusters[a] + self.clusters[b]\n",
    "            del self.clusters[b]\n",
    "            \n",
    "        for i in range(len(self.clusters)):\n",
    "            for j in self.clusters[i]:\n",
    "                self.labels[j] = i\n",
    "            \n",
    "        return self.labels\n",
    "            \n",
    "        \n",
    "    def getLinkageDistance(self,clust1,clust2):\n",
    "        dist = self.distances.get((clust1,clust2))\n",
    "        \n",
    "        if dist == None:\n",
    "            dist = self.distances.get((clust2,clust1))\n",
    "        \n",
    "        if dist == None:\n",
    "            self.distances[(clust1,clust2)] = self.linkage(self.data,clust1,clust2,self.distanceFunction)\n",
    "            dist = self.distances.get((clust1,clust2))\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "            \n",
    "    def singleLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = np.inf\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist = min(dist,distance_function(nodes[i],nodes[j]))\n",
    "        return dist\n",
    "    \n",
    "    def completeLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = -np.inf\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist = max(dist,distance_function(nodes[i],nodes[j]))\n",
    "        return dist\n",
    "    \n",
    "    def averageLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = 0\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist += distance_function(nodes[i],nodes[j])\n",
    "        return dist / (len(clust1)*len(clust2))\n",
    "    \n",
    "    def averageGroupLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = 0\n",
    "        mean1 = []\n",
    "        mean2 = []\n",
    "        for i in clust1:\n",
    "            for j in nodes[i]:\n",
    "                mean1[i]+=j\n",
    "            mean1[i]= mean1[i]/len(nodes)\n",
    "        \n",
    "        for i in clust2:\n",
    "            for j in nodes[i]:\n",
    "                mean2[i]+=j\n",
    "            mean2[i]= mean2[i]/len(nodes)\n",
    "        \n",
    "        return distance_function(mean1,mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with custom Agglomerative:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 3, 1, 1, 3, 2, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Clustering with scikit's Agglomerative:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 1 2 1 2 1 2 2 2 2 1 2 1 2 2 1 2 1 2 1 1\n",
      " 1 1 1 1 1 2 2 2 2 1 2 1 1 1 2 2 2 1 2 2 2 2 2 1 2 2 1 1 3 1 1 3 2 3 1 3 1\n",
      " 1 1 1 1 1 1 3 3 1 1 1 3 1 1 3 1 1 1 3 3 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "\n",
      "Compare with scikit:\n",
      "Yes, it's the same!!\n",
      "\n",
      "Purity:\n",
      "1.0\n",
      "Purity custom Agglomerative:  1.0\n",
      "Purity scikit's Agglomerative:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "distanceFunction = 'eucledian'\n",
    "linkage = 'complete'\n",
    "n_clusters = 4\n",
    "\n",
    "scikitClustering = AgglomerativeClustering(linkage = 'complete',n_clusters = n_clusters)\n",
    "scikitClustering.fit(iris)\n",
    "\n",
    "agl = customAgglomerative(iris,n_clusters,linkage,distanceFunction)\n",
    "\n",
    "print(\"Clustering with custom Agglomerative:\")\n",
    "print(agl.fit())\n",
    "print()\n",
    "print(\"Clustering with scikit's Agglomerative:\")\n",
    "print(scikitClustering.labels_)\n",
    "print()\n",
    "\n",
    "# Compare custom DBSCAN with SCIKIT's\n",
    "print(\"Compare with scikit:\")\n",
    "checkWithScikit(agl.labels,scikitClustering.labels_)\n",
    "print()\n",
    "\n",
    "# Calculate purity\n",
    "print(\"Purity:\")\n",
    "print(calculatePurity(target,agl.labels,n_clusters))\n",
    "print(\"Purity custom Agglomerative: \", calculatePurity(target,agl.labels,n_clusters))\n",
    "print(\"Purity scikit's Agglomerative: \", calculatePurity(target,scikitClustering.labels_,n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementasi ke Dataset Iris\n",
    "\n",
    "Implementasi dataset Iris ke fungsi buatan DBSCAN dan Agglomerative menunjukkan hasil clustering yang sama dengan fungsi bawaan dari sklearn. Oleh karena itu, perhitungan kinerja berupa purity juga menunjukkan nilai yang sama. Pada fungsi buatan KMeans dan KMedoids, terdapat perbedaan nilai pada purity yang diakibatkan oleh terpilihnya centroid awal secara random. Namun, berdasarkan pengujian yang dilakukan, perbedaan nilai purity tersebut rata-rata kurang dari 0,5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembagian Tugas\n",
    "13515055 | Rizky Faramita - KMeans, KMedoids  \n",
    "13515064 | Tasya - Agglomerative  \n",
    "13515140 | Francisco Kenandi Cahyono - DBSCAN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
