{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUBES ML\n",
    "13515055 | Rizky Faramita  \n",
    "13515064 | Tasya  \n",
    "13515140 | Francisco Kenandi Cahyono  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris().data\n",
    "\n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Library & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances,manhattan_distances\n",
    "import numpy as np\n",
    "\n",
    "def calculateEucledianDistance(data):\n",
    "    return euclidean_distances(data)\n",
    "\n",
    "def calculateManhattanDistance(data):\n",
    "    return manhattan_distances(data)\n",
    "\n",
    "def eucledianDistance(a,b):\n",
    "    dist = 0\n",
    "    for i in range(len(a)):\n",
    "        dist += np.power((a[i]-b[i]),2)\n",
    "    return np.sqrt(dist)\n",
    "\n",
    "def manhattanDistance(a,b):\n",
    "    dist = 0\n",
    "    for i in range(len(a)):\n",
    "        dist += abs(a[i]-b[i])\n",
    "    return dist\n",
    "\n",
    "def checkWithScikit(customLabel,scikitLabel):\n",
    "    if len(customLabel) == len(scikitLabel):\n",
    "        same = True\n",
    "        for i in range(len(customLabel)):\n",
    "            if (customLabel[i] != scikitLabel[i]):\n",
    "                print(\"NAY\")\n",
    "                same = False\n",
    "                break\n",
    "        if same: \n",
    "            print(\"YAY\")\n",
    "    else:\n",
    "        print(\"NAY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means merupakan salah satu metode *clustering* yang sensitif terhadap nilai inisialisasi. Pada metode ini, banyaknya kluster yang ingin dibuat sudah diketahui sejak awal.\n",
    "\n",
    "Pada dasarnya, akan diinisialisasi sentroid awal untuk masing-masing kluster. Kemudian, dihitung jarak dari setiap data ke setiap sentroid yang ada. Data tersebut akan dimasukkan ke dalam suatu klulster yang sentroidnya paling dekat dengan data.\n",
    "Kemudian, akan dihitung kembali sentroid setiap kluster dengan cara menghitung *mean*-nya.\n",
    "\n",
    "Langkah di atas akan diulang secara terus-menerus sehingga tidak terjadi perubahan sentroid ketika perhitungan *mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "1. Masukkan banyak kluster yang ingin dibuat dan pilih sentroid awal untuk masing-masing kluster.\n",
    "2. Ulangi: Masukkan (ulang) setiap data ke dalam kluster terdekat, kemudian *update* sentroid setiap kluster dengan cara menghitung *means*-nya.\n",
    "3. Berhenti ketika sentroid dari setiap kluster tidak ada yang berubah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kompleksitas dari algoritma K-Means adalah O(nkt) dengan t adalah jumlah iterasi, k adalah banyaknya kluster, dan n adalah banyaknya data yang ingin dikluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *  \n",
    "import time  \n",
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "# calculate Euclidean distance  \n",
    "def euclDistance(vector1, vector2):  \n",
    "    return sqrt(sum(power(vector2 - vector1, 2)))\n",
    "  \n",
    "# init centroids with random samples  \n",
    "def initCentroids(dataSet, k):  \n",
    "    numSamples, dim = dataSet.shape\n",
    "    centroids = zeros((k, dim))\n",
    "    for i in range(k):  \n",
    "        index = int(random.uniform(0, numSamples))\n",
    "        centroids[i, :] = dataSet[index, :]  \n",
    "    return centroids  \n",
    "  \n",
    "# k-means cluster\n",
    "def kmeans(dataSet, k):  \n",
    "    numSamples = dataSet.shape[0]\n",
    "    clusterAssment = mat(zeros((numSamples, 2)))\n",
    "    clusterChanged = True  \n",
    "  \n",
    "    ## step 1: init centroids  \n",
    "    centroids = initCentroids(dataSet, k)\n",
    "  \n",
    "    while clusterChanged:  \n",
    "        clusterChanged = False  \n",
    "        ## for each sample  \n",
    "        for i in range(numSamples):\n",
    "            minDist  = 100000.0  \n",
    "            minIndex = 0  \n",
    "            ## for each centroid  \n",
    "            ## step 2: find the centroid who is closest\n",
    "            for j in range(k):  \n",
    "                distance = euclDistance(centroids[j, :], dataSet[i, :])  \n",
    "                if distance < minDist:  \n",
    "                    minDist  = distance  \n",
    "                    minIndex = j  \n",
    "              \n",
    "            ## step 3: update its cluster\n",
    "            if clusterAssment[i, 0] != minIndex:  \n",
    "                clusterChanged = True  \n",
    "                clusterAssment[i, :] = minIndex, minDist**2\n",
    "  \n",
    "        ## step 4: update centroids  \n",
    "        for j in range(k):\n",
    "            pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]\n",
    "            centroids[j, :] = mean(pointsInCluster, axis = 0)\n",
    "\n",
    "    return clusterAssment[:,0]\n",
    "    \n",
    "kmeans_result = kmeans(iris_data, 3)\n",
    "print(\"KMeans Iris\",kmeans_result)\n",
    "print(classification_report(iris_target, kmeans_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORTING LIBRARIES #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris_target = iris.target\n",
    "\n",
    "# K-Means Library\n",
    "kmeans_iris = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', \n",
    "                verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto').fit_predict(iris)\n",
    "print(\"KMeans Iris sklearn\", kmeans_iris)\n",
    "print(kmeans_iris.shape[0])\n",
    "print(classification_report(iris_target, kmeans_iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Function Implementation\n",
    "class customDBSCAN:\n",
    "    \n",
    "    def __init__(self,data,e,minPts,distanceFunction):\n",
    "        self.data = data\n",
    "        self.e = e\n",
    "        self.num_data = len(self.data)\n",
    "        self.minPts = minPts\n",
    "        self.labels = []\n",
    "        self.cores = {}\n",
    "        for x in range(len(self.data)):\n",
    "            self.labels.append(-1)\n",
    "        if distanceFunction == 'eucledian':\n",
    "            self.distanceMatrix = calculateEucledianDistance(self.data)\n",
    "        elif distanceFunction == 'manhattan':\n",
    "            self.distanceMatrix = calculateManhattanDistance(self.data)\n",
    "    def findCores(self):\n",
    "        for i in range(self.num_data):\n",
    "            temp_cluster = []\n",
    "            for j in range(self.num_data):\n",
    "                if (self.distanceMatrix[i][j] <= self.e and i != j):\n",
    "                    temp_cluster.append(j)\n",
    "            if(len(temp_cluster) >= self.minPts):\n",
    "                self.cores[i] = temp_cluster\n",
    "    \n",
    "    def dfs_dbscan(self,dict_type_data,current_label):\n",
    "        if(self.labels[dict_type_data] == -1):\n",
    "            self.labels[dict_type_data] = current_label\n",
    "            if dict_type_data in self.cores:\n",
    "                for i in self.cores[dict_type_data]:\n",
    "                    self.dfs_dbscan(i,current_label)\n",
    "        \n",
    "    def fit(self):\n",
    "        label = -1\n",
    "        self.findCores()\n",
    "        for k,v in self.cores.items():\n",
    "            if (self.labels[k] == -1):\n",
    "                label+=1\n",
    "            self.dfs_dbscan(k,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n",
      "YAY\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "distanceFunction = 'eucledian'\n",
    "epsilon = 1\n",
    "minPts = 17\n",
    "\n",
    "# OUR DBSCAN\n",
    "db = customDBSCAN(iris,epsilon,minPts,distanceFunction)\n",
    "db.fit()\n",
    "print(db.labels)\n",
    "\n",
    "# SCIKIT'S DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=epsilon, min_samples=minPts).fit(iris)\n",
    "print(clustering.labels_)\n",
    "\n",
    "checkWithScikit(db.labels,clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGLOMERATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM AGGLOMERATIVE\n",
    "class customAgglomerative:\n",
    "    def __init__(self,data,n_cluster,linkage='single',distanceFunction='eucledian'):\n",
    "        self.data = data\n",
    "        self.n_cluster = n_cluster\n",
    "        \n",
    "        if linkage == 'single':\n",
    "            self.linkage = self.singleLinkage\n",
    "        elif linkage == 'complete':\n",
    "            self.linkage = self.completeLinkage\n",
    "        elif linkage == 'average':\n",
    "            self.linkage = self.averageLinkage\n",
    "        elif linkage == 'average_group':\n",
    "            self.linkage = self.averageGroupLinkage\n",
    "        \n",
    "        if distanceFunction == 'eucledian':\n",
    "            self.distanceFunction = eucledianDistance\n",
    "        elif distanceFunction == 'manhattan':\n",
    "            self.distanceFunction = manhattanDistance\n",
    "        \n",
    "        self.distances = {}\n",
    "        #clusters = list of tuples\n",
    "        self.clusters = [(idx,)for idx in range(len(self.data))]\n",
    "        #label of each data's cluster\n",
    "        self.labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            self.labels.append(i)\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        while len(self.clusters)>self.n_cluster:\n",
    "            a = 0\n",
    "            b = 0\n",
    "            clusterDistance = np.inf\n",
    "            \n",
    "            for i in range(len(self.clusters)):\n",
    "                for j in range(i+1, len(self.clusters)):\n",
    "                    temp_clusterDistance = self.getLinkageDistance(self.clusters[i],self.clusters[j])\n",
    "                    if temp_clusterDistance < clusterDistance:\n",
    "                        a,b,clusterDistance = i,j,temp_clusterDistance\n",
    "                        \n",
    "            self.clusters[a] = self.clusters[a] + self.clusters[b]\n",
    "            del self.clusters[b]\n",
    "            \n",
    "        for i in range(len(self.clusters)):\n",
    "            for j in self.clusters[i]:\n",
    "                self.labels[j] = i\n",
    "            \n",
    "        return self.labels\n",
    "            \n",
    "        \n",
    "    def getLinkageDistance(self,clust1,clust2):\n",
    "        dist = self.distances.get((clust1,clust2))\n",
    "        \n",
    "        if dist == None:\n",
    "            dist = self.distances.get((clust2,clust1))\n",
    "        \n",
    "        if dist == None:\n",
    "            self.distances[(clust1,clust2)] = self.linkage(self.data,clust1,clust2,self.distanceFunction)\n",
    "            dist = self.distances.get((clust1,clust2))\n",
    "        \n",
    "        return dist\n",
    "        \n",
    "            \n",
    "    def singleLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = np.inf\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist = min(dist,distance_function(nodes[i],nodes[j]))\n",
    "        return dist\n",
    "    \n",
    "    def completeLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = -np.inf\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist = max(dist,distance_function(nodes[i],nodes[j]))\n",
    "        return dist\n",
    "    \n",
    "    def averageLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = 0\n",
    "        for i in clust1:\n",
    "            for j in clust2:\n",
    "                dist += distance_function(nodes[i],nodes[j])\n",
    "        return dist / (len(clust1)*len(clust2))\n",
    "    \n",
    "    def averageGroupLinkage(self,nodes,clust1,clust2,distance_function=eucledianDistance):\n",
    "        dist = 0\n",
    "        mean1 = []\n",
    "        mean2 = []\n",
    "        for i in clust1:\n",
    "            for j in nodes[i]:\n",
    "                mean1[i]+=j\n",
    "            mean1[i]= mean1[i]/len(nodes)\n",
    "        \n",
    "        for i in clust2:\n",
    "            for j in nodes[i]:\n",
    "                mean2[i]+=j\n",
    "            mean2[i]= mean2[i]/len(nodes)\n",
    "        \n",
    "        return distance_function(mean1,mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 3, 1, 1, 3, 2, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 1 2 1 2 1 2 2 2 2 1 2 1 2 2 1 2 1 2 1 1\n",
      " 1 1 1 1 1 2 2 2 2 1 2 1 1 1 2 2 2 1 2 2 2 2 2 1 2 2 1 1 3 1 1 3 2 3 1 3 1\n",
      " 1 1 1 1 1 1 3 3 1 1 1 3 1 1 3 1 1 1 3 3 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "YAY\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "distanceFunction = 'eucledian'\n",
    "linkage = 'complete'\n",
    "\n",
    "scikitClustering = AgglomerativeClustering(linkage = 'complete',n_clusters = 4)\n",
    "scikitClustering.fit(iris)\n",
    "\n",
    "agl = customAgglomerative(iris,4,linkage,distanceFunction)\n",
    "\n",
    "print(agl.fit())\n",
    "print(scikitClustering.labels_)\n",
    "\n",
    "checkWithScikit(agl.labels,scikitClustering.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
